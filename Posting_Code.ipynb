{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is for the dataset from: \n",
    "# Health Statistics. (2023). NCHS drug poisoning mortality by county, United States. U.S. Department of Health & Human Services. \n",
    "# Retrieved September 8, 2024, from https://catalog.data.gov/dataset/nchs-drug-poisoning-mortality-by-county-united-states\n",
    "# Out of Elastic Net, XGBoost, and LightGBM modeling, the best performing model was through Elastic Net, with the addition of 3 lag features for death rate and population, for predicting the log-transformed death rate (running the model wihtout the log-transformation has similar results: a slightly lower MAE, but slightly comparitively less accurate results for the other metrics).\n",
    "# As the Elastic Net model showed the most accurate performance, the code is posted for its permutation feature importance and for a plot with the average actual test values versus the model's average predicted test values per test year.\n",
    "# For demonstartion purposes, the code for the best performing XGBoost (which utilizes 1 lag) and LightGBM (3 lags) models (both without any log-transformations of the target variable) are also posted below. \n",
    "\n",
    "#Elastic Net\n",
    "\n",
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "file_path = 'NCHS_-_Drug_Poisoning_Mortality_by_County__United_States.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.drop(columns=['FIPS State', 'FIPS', 'Standard Deviation', 'Lower Confidence Limit', 'Upper Confidence Limit'])\n",
    "df = df.sort_values(by=['Year', 'County'])\n",
    "\n",
    "# Apply log transformation to 'Model-based Death Rate' to prepare for lagged features\n",
    "df['Log_Death_Rate'] = np.log(df['Model-based Death Rate'])\n",
    "\n",
    "# Create lag variables for 'Log_Death_Rate' and 'Population'\n",
    "df['Death_Rate_lag_1'] = df.groupby('County')['Log_Death_Rate'].shift(1)\n",
    "df['Population_lag_1'] = df.groupby('County')['Population'].shift(1)\n",
    "\n",
    "df['Death_Rate_lag_2'] = df.groupby('County')['Log_Death_Rate'].shift(2)\n",
    "df['Population_lag_2'] = df.groupby('County')['Population'].shift(2)\n",
    "\n",
    "df['Death_Rate_lag_3'] = df.groupby('County')['Log_Death_Rate'].shift(3)\n",
    "df['Population_lag_3'] = df.groupby('County')['Population'].shift(3)\n",
    "\n",
    "# Drop rows with NaN in lagged columns\n",
    "df = df.dropna(subset=['Death_Rate_lag_1', 'Population_lag_1', \n",
    "                       'Death_Rate_lag_2', 'Population_lag_2', \n",
    "                       'Death_Rate_lag_3', 'Population_lag_3'])\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
    "categorical_cols = ['State', 'Urban/Rural Category', 'Census Division']\n",
    "encoded_df = pd.DataFrame(onehot_encoder.fit_transform(df[categorical_cols]))\n",
    "encoded_df.columns = onehot_encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "encoded_df = encoded_df.reset_index(drop=True)\n",
    "df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "# Specify exogenous features and target variable\n",
    "exogenous_features = [col for col in df.columns if col not in ['Log_Death_Rate', 'County', 'Model-based Death Rate']]\n",
    "target_variable = 'Log_Death_Rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting based on year ranges\n",
    "train_df = df[df['Year'].between(2006, 2017)]  \n",
    "test_df = df[df['Year'].between(2018, 2021)]  \n",
    "\n",
    "# Define features (X) and target (y) for each split\n",
    "X_train = train_df[exogenous_features]\n",
    "y_train = train_df[target_variable]\n",
    "\n",
    "X_test = test_df[exogenous_features]\n",
    "y_test = test_df[target_variable]\n",
    "\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers for population and death rate features\n",
    "scaler_population = MinMaxScaler()\n",
    "scaler_death_rate_features = MinMaxScaler()\n",
    "scaler_death_rate_target = MinMaxScaler()\n",
    "\n",
    "# Define population and death rate features (including lags)\n",
    "population_features = ['Population', 'Population_lag_1', 'Population_lag_2', 'Population_lag_3']\n",
    "death_rate_features = ['Death_Rate_lag_1', 'Death_Rate_lag_2', 'Death_Rate_lag_3']\n",
    "\n",
    "# Fit and transform population features in X_train using scaler_population\n",
    "X_train[population_features] = scaler_population.fit_transform(X_train[population_features])\n",
    "\n",
    "# Fit and transform death rate lag features in X_train using scaler_death_rate_features\n",
    "X_train[death_rate_features] = scaler_death_rate_features.fit_transform(X_train[death_rate_features])\n",
    "\n",
    "# Fit and transform 'Model-based Death Rate' for y_train using scaler_death_rate_target\n",
    "y_train = scaler_death_rate_target.fit_transform(y_train.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Apply the same transformations to X_test and y_test\n",
    "X_test[population_features] = scaler_population.transform(X_test[population_features])\n",
    "X_test[death_rate_features] = scaler_death_rate_features.transform(X_test[death_rate_features])\n",
    "y_test = scaler_death_rate_target.transform(y_test.to_numpy().reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expanding window cross-validation\n",
    "train_years = np.sort(X_train['Year'].unique())\n",
    "n_splits = len(train_years) - 2\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "expanding_cv = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(train_years[1:])):\n",
    "    train_years_split = np.append(train_years[:1], train_years[train_idx + 1])\n",
    "    val_years_split = train_years[val_idx + 1]\n",
    "    \n",
    "    train_indices = X_train[X_train['Year'].isin(train_years_split)].index\n",
    "    val_indices = X_train[X_train['Year'].isin(val_years_split)].index\n",
    "    expanding_cv.append((train_indices, val_indices))\n",
    "    \n",
    "    # Print the training and validation years for each fold\n",
    "    print(f\"Fold {fold+1}: Training Years: {train_years_split}, Validation Years: {val_years_split}\")\n",
    "\n",
    "\n",
    "# Initialize and perform grid search on ElasticNet\n",
    "param_grid = {\n",
    "    'alpha': [0],  \n",
    "    'l1_ratio': [0]   # After extensive grid searching, no regularization turned out to perform the best\n",
    "}\n",
    "elastic_net = ElasticNet(random_state=42, max_iter=10000)\n",
    "grid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=expanding_cv, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and inverse scaling back to the original Model-based Death Rate scale\n",
    "y_train_pred_log = scaler_death_rate_target.inverse_transform(best_model.predict(X_train).reshape(-1, 1)).flatten()\n",
    "y_test_pred_log = scaler_death_rate_target.inverse_transform(best_model.predict(X_test).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Apply exponential function to reverse log transformation\n",
    "y_train_pred = np.exp(y_train_pred_log)\n",
    "y_test_pred = np.exp(y_test_pred_log)\n",
    "\n",
    "# Reverse transformations on actual target values for y_train and y_test\n",
    "y_train_orig = np.exp(scaler_death_rate_target.inverse_transform(y_train).flatten())\n",
    "y_test_orig = np.exp(scaler_death_rate_target.inverse_transform(y_test).flatten())\n",
    "\n",
    "# Calculate metrics comparing predictions to the original Model-based Death Rate\n",
    "train_metrics = {\n",
    "    \"MSE\": mean_squared_error(y_train_orig, y_train_pred),\n",
    "    \"MAE\": mean_absolute_error(y_train_orig, y_train_pred),\n",
    "    \"R-squared\": r2_score(y_train_orig, y_train_pred),\n",
    "    \"MAPE\": mean_absolute_percentage_error(y_train_orig, y_train_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_train_orig, y_train_pred))\n",
    "}\n",
    "\n",
    "test_metrics = {\n",
    "    \"MSE\": mean_squared_error(y_test_orig, y_test_pred),\n",
    "    \"MAE\": mean_absolute_error(y_test_orig, y_test_pred),\n",
    "    \"R-squared\": r2_score(y_test_orig, y_test_pred),\n",
    "    \"MAPE\": mean_absolute_percentage_error(y_test_orig, y_test_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test_orig, y_test_pred))\n",
    "}\n",
    "\n",
    "print(\"\\nTraining Metrics:\", train_metrics)\n",
    "print(\"Testing Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance on the training set\n",
    "perm_importance_train = permutation_importance(best_model, X_train, y_train, n_repeats=10, random_state=42)\n",
    "\n",
    "# Prepare data for plotting\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance (Mean)': perm_importance_train.importances_mean,\n",
    "    'Importance (Std)': perm_importance_train.importances_std\n",
    "}).sort_values(by='Importance (Mean)', ascending=False)\n",
    "\n",
    "# Plot permutation feature importance\n",
    "plt.figure(figsize=(10, 16))\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance (Mean)'], xerr=feature_importances['Importance (Std)'])\n",
    "plt.xlabel(\"Permutation Importance (Mean Decrease in MSE)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Permutation Feature Importance (Linear Regression with 3 Lags and Log-Transformed Death Rates)\")\n",
    "plt.gca().invert_yaxis()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting the average actual and average predicted values per year for ElasticNet\n",
    "\n",
    "# Use the ElasticNet test predictions and add them to the test DataFrame\n",
    "test_df = test_df.copy()\n",
    "test_df['ElasticNet_Predicted'] = y_test_pred  \n",
    "\n",
    "# Group by Year and calculate the average actual and predicted death rates\n",
    "yearly_avg_elastic_net = test_df.groupby('Year').agg({\n",
    "    'Model-based Death Rate': 'mean',\n",
    "    'ElasticNet_Predicted': 'mean'\n",
    "})\n",
    "\n",
    "# Plot the average actual and predicted values per year for ElasticNet\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(yearly_avg_elastic_net.index, yearly_avg_elastic_net['Model-based Death Rate'], label='Actual', marker='o', color='b')\n",
    "plt.plot(yearly_avg_elastic_net.index, yearly_avg_elastic_net['ElasticNet_Predicted'], label='Predicted', marker='o', color='r')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Death Rate')\n",
    "plt.title('Average Actual vs Predicted Death Rates by Year (Linear Regression with 3 Lags and Log-Transformed Death Rates)')\n",
    "plt.xticks(yearly_avg_elastic_net.index)  \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "file_path = 'NCHS_-_Drug_Poisoning_Mortality_by_County__United_States.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.drop(columns=['FIPS State', 'FIPS', 'Standard Deviation', 'Lower Confidence Limit', 'Upper Confidence Limit'])\n",
    "df = df.sort_values(by=['County', 'Year'])\n",
    "\n",
    "# Create lag variables for 'Model-based Death Rate' and 'Population' (lag of 1 year)\n",
    "df['Death_Rate_lag_1'] = df.groupby('County')['Model-based Death Rate'].shift(1)\n",
    "df['Population_lag_1'] = df.groupby('County')['Population'].shift(1)\n",
    "\n",
    "df = df.dropna(subset=['Death_Rate_lag_1', 'Population_lag_1'])\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
    "\n",
    "# Specify the categorical columns to encode\n",
    "categorical_cols = ['State', 'Urban/Rural Category', 'Census Division']\n",
    "\n",
    "# Apply one-hot encoding to the categorical columns\n",
    "encoded_df = pd.DataFrame(onehot_encoder.fit_transform(df[categorical_cols]))\n",
    "df = df.reset_index(drop=True)\n",
    "encoded_df = encoded_df.reset_index(drop=True)\n",
    "df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "# Select relevant features for exogenous variables (including lag variables)\n",
    "exogenous_features = [col for col in df.columns if col not in ['Model-based Death Rate', 'County']]\n",
    "\n",
    "# The target variable is still the 'Model-based Death Rate'\n",
    "target_variable = 'Model-based Death Rate'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "train_df = df[df['Year'].between(2004, 2017)]  \n",
    "test_df = df[df['Year'].between(2018, 2021)]   \n",
    "\n",
    "# Define features (X) and target (y) for train and test sets\n",
    "X_train = train_df[exogenous_features]\n",
    "y_train = train_df[target_variable]\n",
    "\n",
    "X_test = test_df[exogenous_features]\n",
    "y_test = test_df[target_variable]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom scorers for GridSearchCV\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=False)  # RMSE\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)  # MAE\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)  # MAPE\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=True)  # MSE\n",
    "r2_scorer = make_scorer(r2_score)  # R²\n",
    "\n",
    "# Initialize XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Parameter grid for hyperparameter tuning (best performing hyperparameters found)\n",
    "param_grid = {\n",
    "    'n_estimators': [250],\n",
    "    'learning_rate': [1.1],\n",
    "    'max_depth': [17],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'reg_alpha': [1.9],\n",
    "    'reg_lambda': [1.9],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0.1]\n",
    "}\n",
    "\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Define expanding window cross-validation \n",
    "train_years = np.sort(X_train['Year'].unique())  \n",
    "n_splits = len(train_years) - 2  \n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Track the training and validation years for each fold\n",
    "expanding_cv = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(train_years[1:])):  # Start at the second year\n",
    "    train_years_split = np.append(train_years[:1], train_years[train_idx + 1])\n",
    "    val_years_split = train_years[val_idx + 1]\n",
    "    \n",
    "    train_indices = X_train[X_train['Year'].isin(train_years_split)].index\n",
    "    val_indices = X_train[X_train['Year'].isin(val_years_split)].index\n",
    "    \n",
    "    expanding_cv.append((train_indices, val_indices))\n",
    "    \n",
    "    # Print the training and validation years for each fold\n",
    "    print(f\"Fold {fold+1}: Training Years: {train_years_split}, Validation Years: {val_years_split}\")\n",
    "\n",
    "# GridSearchCV setup and fitting\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring={\n",
    "        'rmse': rmse_scorer, \n",
    "        'mae': mae_scorer, \n",
    "        'mape': mape_scorer, \n",
    "        'mse': mse_scorer, \n",
    "        'r2': r2_scorer\n",
    "    }, \n",
    "    refit='mse',  # refit based on MSE\n",
    "    cv=expanding_cv,  \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training set\n",
    "y_train_pred = grid_search.predict(X_train)\n",
    "\n",
    "# Calculate and print metrics for the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train MAPE: {train_mape * 100}%\")\n",
    "print(f\"Train R²: {train_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test MAE: {mae_test}\")\n",
    "print(f\"Test RMSE: {rmse_test}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(f\"Test MAPE: {mape_test * 100}%\")\n",
    "print(f\"Test R²: {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "\n",
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and preprocess\n",
    "file_path = 'NCHS_-_Drug_Poisoning_Mortality_by_County__United_States.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.drop(columns=['FIPS State', 'FIPS', 'Standard Deviation', 'Lower Confidence Limit', 'Upper Confidence Limit'])\n",
    "df = df.sort_values(by=['Year', 'County'])\n",
    "\n",
    "# Manually rename the 'Urban/Rural Category' and 'Census Division' features to remove white space\n",
    "df = df.rename(columns={\n",
    "    'Urban/Rural Category': 'Urban_Rural_Category',\n",
    "    'Model-based Death Rate': 'Model_based_Death_Rate',\n",
    "    'Census Division': 'Census_Division'\n",
    "})\n",
    "\n",
    "\n",
    "# Create lag variables for 'Population' and 'Model-based Death Rate'\n",
    "df['Population_lag_1'] = df.groupby('County')['Population'].shift(1)\n",
    "df['DeathRate_lag_1'] = df.groupby('County')['Model_based_Death_Rate'].shift(1)\n",
    "\n",
    "df['Population_lag_2'] = df.groupby('County')['Population'].shift(2)\n",
    "df['DeathRate_lag_2'] = df.groupby('County')['Model_based_Death_Rate'].shift(2)\n",
    "\n",
    "df['Population_lag_3'] = df.groupby('County')['Population'].shift(3)\n",
    "df['DeathRate_lag_3'] = df.groupby('County')['Model_based_Death_Rate'].shift(3)\n",
    "\n",
    "\n",
    "# Drop any rows with missing values created by lag\n",
    "df = df.dropna()\n",
    "\n",
    "# Specify the categorical columns \n",
    "categorical_cols = ['State', 'Urban_Rural_Category', 'Census_Division']\n",
    "\n",
    "target_variable = 'Model_based_Death_Rate'\n",
    "\n",
    "# Select relevant features for exogenous variables, including lag features\n",
    "exogenous_features = [col for col in df.columns if col not in ['Model_based_Death_Rate', 'County']]\n",
    "\n",
    "df[categorical_cols] = df[categorical_cols].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "train_df = df[df['Year'].between(2006, 2017)]  \n",
    "test_df = df[df['Year'].between(2018, 2021)]   \n",
    "\n",
    "# Define features (X) and target (y) for train and test sets\n",
    "X_train = train_df[exogenous_features]\n",
    "y_train = train_df[target_variable]\n",
    "\n",
    "X_test = test_df[exogenous_features]\n",
    "y_test = test_df[target_variable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LightGBM regressor\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# Parameter grid for tuning (best-performing hyperparameters found) \n",
    "param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'learning_rate': [1],\n",
    "    'max_depth': [17],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'reg_alpha': [2.2],\n",
    "    'reg_lambda': [2.7],\n",
    "    'min_child_weight': [70],\n",
    "}\n",
    "\n",
    "\n",
    "# Convert categorical columns to 'category' dtype\n",
    "for col in categorical_cols:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Define expanding window cross-validation with at least two years in the first fold\n",
    "train_years = np.sort(X_train['Year'].unique())  \n",
    "n_splits = len(train_years) - 2  \n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Track the training and validation years for each fold\n",
    "expanding_cv = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(train_years[1:])):  # Start at the second year\n",
    "    train_years_split = np.append(train_years[:1], train_years[train_idx + 1])\n",
    "    val_years_split = train_years[val_idx + 1]\n",
    "    \n",
    "    train_indices = X_train[X_train['Year'].isin(train_years_split)].index\n",
    "    val_indices = X_train[X_train['Year'].isin(val_years_split)].index\n",
    "    \n",
    "    expanding_cv.append((train_indices, val_indices))\n",
    "    \n",
    "    # Print the training and validation years for each fold\n",
    "    print(f\"Fold {fold+1}: Training Years: {train_years_split}, Validation Years: {val_years_split}\")\n",
    "\n",
    "# Define custom scorers for GridSearchCV\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=False)  # RMSE\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)  # MAE\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)  # MAPE\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=True)  # MSE\n",
    "r2_scorer = make_scorer(r2_score)  # R²\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning \n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, \n",
    "                           scoring={'rmse': rmse_scorer, 'mae': mae_scorer, 'mape': mape_scorer, 'mse': mse_scorer, 'r2': r2_scorer}, \n",
    "                           refit='mse',  # Refit based on MSE\n",
    "                           cv=expanding_cv,  \n",
    "                           verbose=0)\n",
    "\n",
    "# Fit the model on the training data, passing categorical feature names\n",
    "grid_search.fit(X_train, y_train, categorical_feature=categorical_cols)\n",
    "\n",
    "print(\"Best parameters found by grid search:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training set\n",
    "y_train_pred = grid_search.predict(X_train)\n",
    "\n",
    "# Calculate and print metrics for the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train MAPE: {train_mape * 100}%\")\n",
    "print(f\"Train R²: {train_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate and print metrics for the test set\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MAPE: {test_mape * 100}%\")\n",
    "print(f\"Test R²: {test_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
